{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to display all the header tags from\n",
    "‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscraping(link,tag):\n",
    "    wiki_page=requests.get(link)\n",
    "    \n",
    "    wiki=BeautifulSoup(wiki_page.content)\n",
    "    \n",
    "    titles=wiki.find_all(tag)\n",
    "    \n",
    "    for title in titles:\n",
    "        print(title.text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_link='https://en.wikipedia.org/wiki/Main_Page'\n",
    "tag='h2'\n",
    "tags=['h1','h2','h3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headings in h2 tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscraping(wiki_link,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heading in h1,h2, and h3 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscraping(wiki_link,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name,\n",
    "IMDB rating, Year of release) and save it in form of a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_link='https://www.imdb.com/chart/top/?ref_=nv_mv_250'\n",
    "tag='td'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscraping_imdb(link,tag):\n",
    "    imdb_page=requests.get(link)\n",
    "    imdb=BeautifulSoup(imdb_page.content)\n",
    "    \n",
    "    names=[]\n",
    "    years=[]\n",
    "    for x in imdb.find_all(tag,class_='titleColumn')[:100]:\n",
    "        names.append(x.find('a').text)\n",
    "        years.append(re.sub('[()]','',x.find('span').text))\n",
    "        \n",
    "    ratings=[]\n",
    "    for x in imdb.find_all('td',class_='ratingColumn imdbRating')[:100]:\n",
    "        ratings.append(x.text.replace('\\n',''))\n",
    "        \n",
    "    df=pd.DataFrame({'Movie':names,'Rating':ratings,'Year_of_Release':years,})\n",
    "    print(df)\n",
    "    df.to_csv('imdb_top100movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscraping_imdb(imdb_link,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e.\n",
    "Name, IMDB rating, Year of release) and save it in form of a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ind_link='https://www.imdb.com/india/top-rated-indian-movies/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=8a7876cd-2844-4017-846a-2c0876945b7b&pf_rd_r=QA4JGCD0VVTMXZ85RHJ8&pf_rd_s=right-5&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_india_tr_rhs_1'\n",
    "tag='td'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscraping_imdb_ind(link,tag):\n",
    "    imdb_ind_page=requests.get(link)\n",
    "    imdb_ind=BeautifulSoup(imdb_ind_page.content)\n",
    "    \n",
    "    names=[]\n",
    "    years=[]\n",
    "    for x in imdb_ind.find_all(tag,class_='titleColumn')[:100]:\n",
    "        names.append(x.find('a').text)\n",
    "        years.append(re.sub('[()]','',x.find('span').text))\n",
    "        \n",
    "    ratings=[]\n",
    "    for x in imdb_ind.find_all('td',class_='ratingColumn imdbRating')[:100]:\n",
    "        ratings.append(x.text.replace('\\n',''))\n",
    "        \n",
    "    df=pd.DataFrame({'Movie':names,'Rating':ratings,'Year_of_Release':years,})\n",
    "    print(df)\n",
    "    df.to_csv('imdb_ind_top100movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscraping_imdb_ind(imdb_ind_link,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to scrap book name, author name, genre and book review of\n",
    "any 5 books from ‘www.bookpage.com’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk_link='https://bookpage.com/reviews'\n",
    "tags=['h4','p','p','div']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookpage_scraping(link,tags):\n",
    "    bk_page=requests.get(link)\n",
    "    bk=BeautifulSoup(bk_page.content)\n",
    "    driver=webdriver.Chrome(executable_path='D:\\Data Science\\Web Scraping\\chromedriver.exe')\n",
    "    driver.get(link)\n",
    "    itag=iter(tags)\n",
    "    \n",
    "    tag=next(itag)\n",
    "    bk_titles=[]\n",
    "    for x in bk.find_all(tag,class_='italic')[3:8]:\n",
    "        bk_titles.append(x.text.replace('\\n',''))\n",
    "        \n",
    "    tag=next(itag)    \n",
    "    bk_authors=[]\n",
    "    for x in bk.find_all(tag,class_='sans bold')[3:8]:\n",
    "        bk_authors.append(x.text.replace('\\n',''))\n",
    "    \n",
    "    tag=next(itag)\n",
    "    bk_genres=[]\n",
    "    for x in bk.find_all(tag,class_='genre-links hidden-phone')[3:8]:\n",
    "        bk_genres.append(x.text.replace('\\n',''))\n",
    "        \n",
    "    tag=next(itag)\n",
    "    bk_reviews=[]\n",
    "    for x in bk_titles:\n",
    "        driver.find_element_by_link_text(x).click()\n",
    "        time.sleep(0.8)\n",
    "        current_bk_page=requests.get(driver.current_url)\n",
    "        driver.get(link)\n",
    "        time.sleep(0.8)\n",
    "\n",
    "        current_bk=BeautifulSoup(current_bk_page.content)\n",
    "        bk_reviews.append(current_bk.find(tag,class_='article-body').text.replace('\\n',''))\n",
    "        \n",
    "    df=pd.DataFrame({'Book':bk_titles,'Author':bk_authors,'Genre':bk_genres,'Review':bk_reviews})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookpage_scraping(bk_link,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You\n",
    "have to scrape:\n",
    "- Top 10 ODI teams in men’s cricket along with the records for matches, points and\n",
    "rating.\n",
    "- Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "- Top 10 ODI bowlers along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc_link='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "tag='tr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_scraping(link,tag):\n",
    "    icc_page=requests.get(link)\n",
    "    icc_odi=BeautifulSoup(icc_page.content)\n",
    "    \n",
    "    ranks=[]\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    ranks.append(int(icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--pos').text))\n",
    "    teams.append(icc_odi.find(tag,class_='rankings-block__banner').find('span',class_='u-hide-phablet').text)\n",
    "    matches.append(int(icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--matches').text))\n",
    "    points.append(icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--points').text)\n",
    "    ratings.append(int(re.sub('[\\n ]','',icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--rating u-text-right').text)))\n",
    "    \n",
    "    for team in icc_odi.find_all(tag,class_='table-body')[:9]:\n",
    "        ranks.append(int(re.split('\\n',team.text)[1]))\n",
    "        teams.append(re.split('\\n',team.text)[4])\n",
    "        matches.append(int(re.split('\\n',team.text)[7]))\n",
    "        points.append(re.split('\\n',team.text)[8])\n",
    "        ratings.append(int(re.split('\\n',team.text)[9]))\n",
    "    \n",
    "    df=pd.DataFrame({'Rank':ranks,'Team':teams,'Matches':matches,'Point':points,'Ratings':ratings})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc_scraping(icc_link,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You\n",
    "have to scrape:\n",
    "- Top 10 ODI teams in women’s cricket along with the records for matches, points\n",
    "and rating.\n",
    "- Top 10 women’s ODI players along with the records of their team and rating.\n",
    "- Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wicc_link='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "tag='tr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wicc_scraping(link,tag):\n",
    "    icc_page=requests.get(link)\n",
    "    icc_odi=BeautifulSoup(icc_page.content)\n",
    "    \n",
    "    ranks=[]\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    ranks.append(int(icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--pos').text))\n",
    "    teams.append(icc_odi.find(tag,class_='rankings-block__banner').find('span',class_='u-hide-phablet').text)\n",
    "    matches.append(int(icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--matches').text))\n",
    "    points.append(icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--points').text)\n",
    "    ratings.append(int(re.sub('[\\n ]','',icc_odi.find(tag,class_='rankings-block__banner').find('td',class_='rankings-block__banner--rating u-text-right').text)))\n",
    "    \n",
    "    for team in icc_odi.find_all(tag,class_='table-body')[:9]:\n",
    "        ranks.append(int(re.split('\\n',team.text)[1]))\n",
    "        teams.append(re.split('\\n',team.text)[4])\n",
    "        matches.append(int(re.split('\\n',team.text)[7]))\n",
    "        points.append(re.split('\\n',team.text)[8])\n",
    "        ratings.append(int(re.split('\\n',team.text)[9]))\n",
    "    \n",
    "    df=pd.DataFrame({'Rank':ranks,'Team':teams,'Matches':matches,'Point':points,'Ratings':ratings})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wicc_scraping(wicc_link,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to scrape details of all the mobile phones under Rs. 20,000\n",
    "listed on Amazon.in. The scraped data should include Product Name, Price, Image URL\n",
    "and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_page=requests.get('https://www.amazon.in/s?bbn=1389401031&rh=n%3A1389401031%2Cp_36%3A1318506031&dc&qid=1620500039&rnid=1318502031&ref=lp_1389401031_nr_p_36_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hence Response is not 200 we cannot scrap this page data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Flipkart data Instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_link='https://www.flipkart.com/search?sid=tyy%2C4io&otracker=CLP_Filters&p%5B%5D=facets.price_range.from%3DMin&p%5B%5D=facets.price_range.to%3D20000&p%5B%5D=facets.rating%255B%255D%3D4%25E2%2598%2585%2B%2526%2Babove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_scraping(links):\n",
    "    flipkart_page=requests.get(links)\n",
    "    flipkart=BeautifulSoup(flipkart_page.content)\n",
    "    driver=webdriver.Chrome(executable_path='D:\\Data Science\\Web Scraping\\chromedriver.exe')\n",
    "    driver.get(links)\n",
    "    \n",
    "    mobiles=[]\n",
    "    for x in flipkart.find_all('div',class_='_2kHMtA'):\n",
    "        mobiles.append(x.find('div',class_='_4rR01T').text)\n",
    "        \n",
    "    prices=[]\n",
    "    for x in flipkart.find_all('div',class_='_2kHMtA'):\n",
    "        prices.append(x.find('div',class_='_30jeq3 _1_WHN1').text)\n",
    "        \n",
    "    urls=[]    \n",
    "    for link in driver.find_elements_by_xpath('//div[@class=\"_2kHMtA\"]/a'):\n",
    "        driver.get(link.get_attribute('href'))\n",
    "        time.sleep(.8)\n",
    "        urls.append(driver.find_element_by_xpath('//div[@class=\"CXW8mj _3nMexc\"]/img').get_attribute('src'))\n",
    "\n",
    "        driver.get(links)\n",
    "        time.sleep(.8)\n",
    "        \n",
    "    df=pd.DataFrame({'Mobile':mobiles,'Price':prices,'Image_URL':urls})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_scraping(flip_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to extract information about the local weather from the National\n",
    "Weather Service website of USA, https://www.weather.gov/ for the city, San\n",
    "Francisco. You need to extract data about 7 day extended forecast display for the city.\n",
    "The data should include period, short description, temperature and description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_link='https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.YJep3bUzZPY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_scraping(link):\n",
    "    wheather_page=requests.get(link)\n",
    "    wheather=BeautifulSoup(wheather_page.content)\n",
    "    \n",
    "    periods=[]\n",
    "    temps=[]\n",
    "    \n",
    "    for x in wheather.find_all('li',class_='forecast-tombstone'):\n",
    "        periods.append(x.find('p').text)\n",
    "        temps.append(x.find('p',class_='temp temp-low').text)\n",
    "        \n",
    "    decps=[]\n",
    "    \n",
    "    for x in wheather.find_all('div',class_='row row-odd row-forecast'):\n",
    "        decps.append(x.find('div',class_='col-sm-10 forecast-text').text)\n",
    "        \n",
    "    df=pd.DataFrame({'Period':periods,'Temperature':temps,'Description':decps})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_scraping(weather_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
